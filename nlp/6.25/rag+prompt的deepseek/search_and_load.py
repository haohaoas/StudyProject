import requests
from bs4 import BeautifulSoup
from url_search import search_urls

def is_page_valid(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code != 200:
            print(f" ç½‘é¡µè¯·æ±‚å¤±è´¥ï¼š{url} çŠ¶æ€ç ï¼š{resp.status_code}")
            return False

        soup = BeautifulSoup(resp.content, "html.parser")
        text = soup.get_text(separator='\n').strip()

        if len(text) < 500:
            print(f"ç½‘é¡µå†…å®¹å¤ªå°‘ï¼š{url}")
            return False

        print(f" ç½‘é¡µæœ‰æ•ˆï¼š{url}")
        return True

    except Exception as e:
        print(f" ç½‘é¡µè¯·æ±‚å¼‚å¸¸ï¼š{url}ï¼ŒåŸå› ï¼š{e}")
        return False


def load_web_docs(url_list):
    docs = []
    for url in url_list:
        if is_page_valid(url):
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                resp = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(resp.content, "html.parser")
                text = soup.get_text(separator='\n').strip()

                docs.append(text)
                print(f" æˆåŠŸæŠ“å–ï¼š{url}")

            except Exception as e:
                print(f" æŠ“å–å¤±è´¥ï¼š{url}ï¼ŒåŸå› ï¼š{e}")
        else:
            print(f" è·³è¿‡æ— æ•ˆé¡µé¢ï¼š{url}")

    return docs


def auto_build_docs(query):
    url_list = search_urls(query)
    print(f"ğŸ” æœç´¢åˆ°çš„URLåˆ—è¡¨ï¼š{url_list}")

    if not url_list:
        print(" æ²¡æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œå°è¯•æ›´æ¢å…³é”®è¯ã€‚")
        return []

    docs = load_web_docs(url_list)
    print(f" æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£ã€‚")

    return docs